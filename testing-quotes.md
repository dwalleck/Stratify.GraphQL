- I think the main lesson that I’ve learned about testing is to treat your tests as a software engineering artifact just like the rest of your code. They should be well documented and easy to modify. If they test an implementation detail then changing the tests should be as easy as changing that aspect of the implementation. When I change the implementation and I get test failures, I should be able to look at that test and see a comment telling me precisely what it’s testing and why. I can then decide if I want to update it for the new implementation or just delete it.
  [[Test against what won't change | Lobsters](https://lobste.rs/s/8p0q7g/test_against_what_won_t_change)]
- A test name, whether it's a method name or a label, is an opportunity to communicate with the reader of the code. You can communicate via code, via names, via comments, and so on. A test name is more like a mandatory comment than a normal method name. [Some Thoughts on Naming Tests]([Some thoughts on naming tests (ploeh.dk)](https://blog.ploeh.dk/2022/06/13/some-thoughts-on-naming-tests/))
- When the test fails it also should provide enough information to understand which behaviour failed, where it failed, and (at least superficially) _why_ it failed. If the only output of a failing test is just a binary value like “FAIL”, that test is only giving one bit of information to the developer. A good test framework will also print the test name and a call stack.
- First let’s get clarity between a flaky test and a failing test
	-   A **_failing_** **test** is a test that fails consistently across multiple reruns (Note: this is not always true. If the environment that a test was written in differs from the environment used in CI, there can be differences in behavior such as page load times that would cause the test to always fail in CI and be a flakey test)
	-   A **_flaky_** **test** is a test that eventually passes across reruns if the test reruns multiple times
- Manual triaging of flaky tests takes ~ 28 mins/PR
- In reality, most UI tests end up being unstable because developers forget to follow two best practices in testing: test only what you need to test and keep full control of test state. [Android UI Automation: Part 1, Building Trust - Slack Engineering](https://slack.engineering/android-ui-automation-part-1-building-trust/)
- If your test is meant to validate functionality of the settings screen, it should not navigate through the login process, open the navigation drawer, press the overflow button, and only then start the actual test interactions related to settings. That would make the test more difficult to author, slower to execute, and more likely to fail in code paths that are unrelated to the functionality under test. [Android UI Automation: Part 1, Building Trust - Slack Engineering](https://slack.engineering/android-ui-automation-part-1-building-trust/)